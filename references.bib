@article{hutter2018TCGA, 
 title={The Cancer Genome Atlas: creating lasting value beyond its data}, 
 author={Hutter, Carolyn and Zenklusen, Jean Claude}, 
 journal={Cell}, 
 volume={173}, 
 number={2}, 
 pages={283--285}, 
 year={2018}, 
 publisher={Elsevier} 
}

@article{gliozzo2022heterogeneous,
  title={Heterogeneous data integration methods for patient similarity networks},
  author={Gliozzo, Jessica and Mesiti, Marco and Notaro, Marco and Petrini, Alessandro and Patak, Alex and Puertas-Gallardo, Antonio and Paccanaro, Alberto and Valentini, Giorgio and Casiraghi, Elena},
  journal={Briefings in Bioinformatics},
  volume={23},
  number={4},
  pages={bbac207},
  year={2022},
  publisher={Oxford University Press}
}

@article{wang2014similarity,
  title={Similarity network fusion for aggregating data types on a genomic scale},
  author={Wang, Bo and Mezlini, Aziz M and Demir, Feyyaz and Fiume, Marc and Tu, Zhuowen and Brudno, Michael and Haibe-Kains, Benjamin and Goldenberg, Anna},
  journal={Nature methods},
  volume={11},
  number={3},
  pages={333--337},
  year={2014},
  publisher={Nature Publishing Group US New York}
}

@article{saria2015subtyping,
  title={Subtyping: What it is and its role in precision medicine},
  author={Saria, Suchi and Goldenberg, Anna},
  journal={IEEE Intelligent Systems},
  volume={30},
  number={4},
  pages={70--75},
  year={2015},
  publisher={IEEE}
}

@article{von2007SP,
  title={A tutorial on spectral clustering},
  author={Von Luxburg, Ulrike},
  journal={Statistics and computing},
  volume={17},
  pages={395--416},
  year={2007},
  publisher={Springer}
}

@book{wagner2007comparing,
  title={Comparing clusterings: an overview},
  author={Wagner, Silke and Wagner, Dorothea},
  year={2007},
  publisher={Universit{\"a}t Karlsruhe, Fakult{\"a}t f{\"u}r Informatik Karlsruhe}
}

@article{abeshouse2015molecularPRAD,
  title={The molecular taxonomy of primary prostate cancer},
  author={Abeshouse, Adam and Ahn, Jaeil and Akbani, Rehan and Ally, Adrian and Amin, Samirkumar and Andry, Christopher D and Annala, Matti and Aprikian, Armen and Armenia, Joshua and Arora, Arshi and others},
  journal={Cell},
  volume={163},
  number={4},
  pages={1011--1025},
  year={2015},
  publisher={Elsevier}
}

@article{shen2009integrative,
  title={Integrative clustering of multiple genomic data types using a joint latent variable model with application to breast and lung cancer subtype analysis},
  author={Shen, Ronglai and Olshen, Adam B and Ladanyi, Marc},
  journal={Bioinformatics},
  volume={25},
  number={22},
  pages={2906--2912},
  year={2009},
  publisher={Oxford University Press}
}

@Inbook{Hastie2009ElementsStatisticalLearning,
author="Hastie, Trevor
and Tibshirani, Robert
and Friedman, Jerome",
title="Prototype Methods and Nearest-Neighbors",
bookTitle="The Elements of Statistical Learning: Data Mining, Inference, and Prediction",
year="2009",
publisher="Springer New York",
address="New York, NY",
pages="459--483",
section = "13.2.1",
isbn="978-0-387-84858-7",
doi="10.1007/978-0-387-84858-7_13",
url="https://doi.org/10.1007/978-0-387-84858-7_13"
}

@book{chung1997SpectralGraphTheory,
language = {eng},
publisher = {American Mathematical Society},
series = {Regional conference series in mathematics},
title = {Spectral graph theory / Fan R. K. Chung},
year = {1997},
author = {Chung , Fan R. K. and Chung,, Fan R. K. and CBMS conference on recent advances in spectral graph theory},
address = {Providence},
booktitle = {Spectral graph theory},
isbn = {08-218-0315-8},
}

@inproceedings{Ng2002spectralClustering,
author = {Ng, Andrew Y. and Jordan, Michael I. and Weiss, Yair},
title = {On Spectral Clustering: Analysis and an Algorithm},
year = {2001},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Despite many empirical successes of spectral clustering methods— algorithms that cluster points using eigenvectors of matrices derived from the data—there are several unresolved issues. First. there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems.},
booktitle = {Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic},
pages = {849–856},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'01}
}

@article{StrehlA2003Ce-A,
journal = {Journal of machine learning research},
language = {eng},
number = {3},
pages = {583-617},
title = {Cluster ensembles - A knowledge reuse framework for combining multiple partitions},
volume = {3},
year = {2003},
abstract = {This paper introduces the problem of combining multiple partitionings of a set of objects into a single consolidated clustering without accessing the features or algorithms that determined these partitionings. We first identify several application scenarios for the resultant `knowledge reuse' framework that we call cluster ensembles. The cluster ensemble problem is then formalized as a combinatorial optimization problem in terms of shared mutual information. In addition to a direct maximization approach, we propose three effective and efficient techniques for obtaining high-quality combiners (consensus functions). The first combiner induces a similarity measure from the partitionings and then reclusters the objects. The second combiner is based on hypergraph partitioning. The third one collapses groups of clusters into meta-clusters, which then compete for each object to determine the combined clustering. Due to the low computational costs of our techniques, it is quite feasible to use a supra-consensus function that evaluates all three approaches against the objective function and picks the best solution for a given situation. We evaluate the effectiveness of cluster ensembles in three qualitatively different application scenarios: (i) where the original clusters were formed based on non-identical sets of features, (ii) where the original clustering algorithms worked on non-identical sets of objects, and (iii) where a common data-set is used and the main purpose of combining multiple clusterings is to improve the quality and robustness of the solution. Promising results are obtained in all three situations for synthetic as well as real data-sets.},
author = {Strehl, A and Ghosh, J},
issn = {1532-4435},
}

@inbook{PAM,
  author = {Kaufman, Leonard and Rousseeuw, Peter J. },
  publisher = {John Wiley \& Sons, Ltd},
  isbn = {9780470316801},
  title = {Partitioning Around Medoids (Program PAM)},
  booktitle = {Finding Groups in Data},
  chapter = {2},
  pages = {68-125},
  doi = {https://doi.org/10.1002/9780470316801.ch2},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470316801.ch2},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470316801.ch2},
  year = {1990},
  keywords = {medoids, representative objects, central memory, partitioning around       medoids, graphical representation},
  abstract = {Summary The prelims comprise: Short Description of the Method How to Use   the Program PAM Examples More on the Algorithm and the Program Related Methods and     References}
}