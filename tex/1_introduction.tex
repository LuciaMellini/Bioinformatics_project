\chapter{Introduction}
The objective of the project is to discover disease subtypes of Prostate adenocarcinoma from a dataset coming from The Cancer Genome Atlas (TCGA)\cite{hutter2018TCGA}. For the examined cancer we take into consideration the dataset with code "\textit{PRAD}". In particular we will consider the subtypes identified by The Cancer Genome Atlas Research Network\cite{abeshouse2015molecularPRAD}. We will be looking at a subset of the types of data analysed in\cite{abeshouse2015molecularPRAD}, namely the biomolecules  obtained from mRNA and miRNA sequencing, and reverse-phase protein array of the samples. The Cancer Genome Atlas Research Network performed unsupervised
clustering of data from each molecular platform, as well as integrative clustering using iCluster\cite{shen2009integrative}. The latter approach is more ideal seen that it allows joint inference from multi-omics data by generating a single integrated cluster assignment through simultaneously capturing patterns of genomic alterations. Instead, identifying tumor subtypes by separately clustering each data view requires to manually integrate the results.
The analyses with both individual and integrative clustering of the TCGA Prostate adenocarcinoma dataset uncovered both known and novel associations, with 74\% of all tumors being assignable to one of seven molecular classes. These are based on distinct oncogenic drivers: fusions involving (1) ERG, (2) ETV1, (3) ETV4, or (4) FLI1; mutations in (5) SPOP
or (6) FOXA1; or (7) IDH1 mutations. Thus, more generally, the tumors can be subdivided into 3 disease subtypes. We will try to predict the membership of a sample in the multi-omics dataset to a class by applying different clustering algorithms.

\section{Data integration}\label{introduction_dataInt}
We could try and classify each data view of the multi-omics dataset (in our case mRNA, miRNA and protein expression) into subtypes. Nevertheless the disease subtpyes would be better determined if all of the molecular data were taken into account simultaneously. So, we want to fuse the different types of data from the multi-omics dataset into a unique object on which it is possible to evaluate the distance between the samples. We start by finding the similarity matrix for each data source using exponential Euclidian distance, also called an affinity matrix.
More formally the similarity matrix among samples of a data source $s$ is given by:
\begin{equation} \label{eq:scaled_exponential_sim}
    W^{(s)}(i,j) = exp \left(- \frac{\rho(x_i,x_j)^2}{\mu \varepsilon_{ij}}\right)
\end{equation}
where:
\begin{description}
    \item [$\rho(x_i, x_j)$] is the Euclidean  distance  between samples \(x_i\) and \(x_j\)
    \item [$\mu$] is a parameter
    \item [$\varepsilon_{i,j}$] is a scaling factor: $\varepsilon_{i,j} = \frac{mean(\rho(x_i, N_i)) + mean(\rho(x_j, N_j)) + \rho(x_i, x_j)}{3}$, where  $mean(\rho(x_i, N_i))$ is the average value of the distances between $x_i$ and each of its neighbours
\end{description}
These similarity matrices for each data source $s$ specify the distance between all possible pairs of samples. This format of the data allows the application of data integration methods, like the ones described in Section \ref{dataInt_mean} and \ref{dataInt_SNF}.

\subsection{Mean} \label{dataInt_mean}
In this case we compute the integrated matrix by averaging component-wise the original three distance matrices obtained from data regarding mRNA, miRNA and protein espression. This method is very naÃ¯ve since it considers strictly the similarity between pairs of samples, without having a global view of the distances.

\subsection{SNF} \label{dataInt_SNF}
The SNF\cite{wang2014similarity} approach uses networks of samples as basis for integration. It consists of two steps:
\begin{itemize}
    \item construct a sample-similarity network for each data type
    \item integrate the networks into a single similarity network using a nonlinear combination method
\end{itemize}
The fused network captures shared and complementary information from the different data sources, hus offering insight into how informative each data view is to the similarity between samples.

The first step in the SNF algorithm is the construction of a similarity matrix among samples for each data source $s$, as described in Section \ref{introduction_dataInt}. Then, other two matrices are derived from $W^{(s)}(i,j)$. One is a ``global" similarity matrix $P^{(s)}$ which captures the overall relationships between samples:

\begin{equation}\label{eq:global_kernel}
    G^{(s)}(i,j) = 
    \begin{dcases}
        \frac{W^{(s)}(i,j)}{2 \sum_{k \neq i} W^{(s)}(i,k)} & \text{ if $j \neq i$}\\
        \frac{1}{2} & \text{ if $j = i$}\\
    \end{dcases}       
\end{equation}


For this equation the property $\sum_{j} P(i,j)=1$ holds.

The other one is a ``local" similarity matrix $S^{(s)}$, that considers only local similarities in the neighbuorhood of each individual:

\begin{equation}
\label{eq:local_kernel}
    L^{(s)}(i,j) = 
    \begin{dcases}
      \frac{W^{(s)}(i,j)}{\sum_{k \in N_i} W^{(s)}(i,k)} & \text{ if $j \in N_i$}\\
      0 & \text{otherwise}\\
    \end{dcases}       
\end{equation}

where $N_i = \{ x_k | x_k \in kNN(x_i) \cup \{ x_i \}\}$.

For all $s\in S$ data views, the matrices $W^{(s)}$, $L^{(s)}$ and $G^{(s)}$ are constructed.  The similarities are diffused through the $G^{(s)}$ until convergence when in step $T$ it is similar to the matrix computed in step $T-1$.  
In the simplest case, when $|S|=2$, we have $G_t^{(s)}$ that refers to the matrix $P^{(s)}$ for data $s \in \{ s_1,s_2\}$ at time $t$.
In this case, the following recursive updating formulas describes the diffusion process:

\begin{equation}
    \label{eq:update}
    \begin{aligned}
        G^{(s_1)}_{t+1}=L^{(s_1)} \times G^{(s_2)}_{t} \times L^{(s_1)\top} \\
        G^{(s_2)}_{t+1}=L^{(s_2)} \times G^{(s_1)}_{t} \times L^{(s_2)\top}  
    \end{aligned}
\end{equation}

In other words $G^{(s_1)}$ is updated by using $L^{(s_1)}$ from the same data source but $G^{(s_2)}$ from a different view and vice-versa.

SNF can be easily extended to $|S| > 2$ data sources.

The final integrated matrix $G^{(c)}$ is computed by averaging:

\begin{equation}
    \label{eq:consensus}
    G^{(c)} = \frac{1}{|S|} \sum_{k=1}^{|S|} G_T^{(k)}
\end{equation}


\section{Clustering algorithms}