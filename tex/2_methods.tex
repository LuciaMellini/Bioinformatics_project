\chapter{Methods}
\section{Data pre-processing}\label{methods_preProcessing}
In this section we list all the modifications that have been made to the original multi-omics dataset downloaded from TCGA.\newline

For the following paragraphs it is helpful to understand the format of TCGA barcodes\footnote{More information about TCGA barcodes can be found at \url{https://docs.gdc.cancer.gov/Encyclopedia/pages/TCGA_Barcode/}}, that describe the nature of the samples. These barcodes contain a set of codes that specify various information about the relative sample. A barcode has 28 characters that describe:
\begin{equation*}
    \underbrace{\text{\footnotesize AAAA}}_\text{project}-
    \underbrace{\text{00}}_\text{TSS}-
    \underbrace{\text{0000}}_\text{participant}-
    \underbrace{\text{00}}_\text{sample}
    \underbrace{\text{A}}_\text{vial}-
    \underbrace{\text{00}}_\text{portion}
    \underbrace{\text{A}}_\text{analyte}-
    \underbrace{\text{0000}}_\text{plate}-
    \underbrace{\text{00}}_\text{center}
\end{equation*}
\paragraph{Primary solid tumors}
We only look at samples regarding primary solid tumors, so those that do not concern e.g. metastatic, solid, blood derived tumors. The type of sample analyzed is specified by the sample barcode characters ``\texttt{01}" in position 13 and 14.
For the dataset in question the length of the barcodes is one character shorter for the data view \textit{PRAD\_RPPAArray}, in fact the specification of analyte information for the samples is missing in the 20th position of the barcode. Seen that in the evaluation phase we consider only the first 12 characters of the barcode, concerning a specific patient (fields Project, Tissue Source Site (TSS) and Participant), this difference is irrelevant for the scope of this project.
\paragraph{Replicates} We do not want to avoid working with replicates for the same sample, so we check that the barcode, the identifier for the sample, are unique in the multi-omics dataset. It emerges that our dataset doesn't contain replicated samples.
\paragraph{FFPE} Also, we want to take into account only samples which biopsies have been frozen. In fact, formalin-fixed, paraffin-embedded (FFPE) biopsies are preserved worse.
\paragraph{All data views} Obviously we want to look at the samples from all three points of view, so we keep only those that are present in all three considered data views in the multi-omics dataset.
\paragraph{Missing values} In addition we get rid of features having missing values. The samples kept until now in the mRNA and miRNA data views don't contain \textit{NA}s, but the protein expression features were not completely ``clean''.\
\paragraph{Highest variance} Subsequently we select features having the most variance across samples. The idea is that such data brings more information, and so is more relevant. In this case we consider only the first 100 features with highest variance between the samples.
\paragraph{Normalization} We then standardize such features with z-score, by applying for each value $v$
\begin{equation*}
    \frac{(v-\mu)}{\sigma}
\end{equation*}
where $\mu$ and $\sigma$ are respectively the mean and standard deviation of the values for some attribute across samples.
\paragraph{Trim barcodes} We are only interested in the barcode information about a specific individual, so we retain only the first 12 characters describing the patient. In particular we keep the sample's Project, TSS and Participant.
\paragraph{Samples with specified subtype} We consider only the samples in the multi-omics that possess information about the subtype data obtained through iCluster. Seen that the subtype samples are a subset of those in the multi-omics data, it is sufficient to keep only that subset of patients.

\section{Data integration}\label{methods_dataInt}
We could try and classify each data view of the multi-omics dataset (in our case mRNA, miRNA and protein expression) into subtypes. Nevertheless the disease subtypes would be better determined if all of the molecular data were taken into account simultaneously. So, we want to fuse the different types of data from the multi-omics dataset into a unique object on which it is possible to evaluate the distance between the samples. We start by finding the similarity matrix for each data source using exponential Euclidean distance, also called an affinity matrix.
More formally the similarity matrix among samples of a data source $s$ is given by:
\begin{equation*}
    W^{(s)}(i,j) = exp \left(- \frac{\rho(x_i,x_j)^2}{\mu \varepsilon_{ij}}\right)
\end{equation*}
where:
\begin{description}
    \item [\boldmath$\rho(x_i, x_j)$] is the Euclidean  distance  between samples \(x_i\) and \(x_j\),
    \item [\boldmath$\mu$] is a parameter,
    \item [\boldmath$\varepsilon_{i,j}$] is a scaling factor: $\varepsilon_{i,j} = \frac{\mu(\rho(x_i, N_i)) + \mu(\rho(x_j, N_j)) + \rho(x_i, x_j)}{3}$, where  $\mu(\rho(x_i, N_i))$ is the average value of the distances between $x_i$ and each of its neighbours.
\end{description}
The similarity matrices for each data source $s$ specify the distance between all possible pairs of samples. This format of the data allows to apply data integration methods, like the the ones described in Section~\ref{dataInt_mean} and~\ref{dataInt_SNF}.

\subsection{Mean} \label{dataInt_mean}
In this case we compute the integrated matrix by averaging component-wise the original three distance matrices obtained from data regarding mRNA, miRNA and protein expression. This method is very naÃ¯ve since it considers strictly the similarity between pairs of samples, without having a global view of the distances. The simplicity of this approach is also reflected in the poorer clustering results with respect to the other used approach.

\subsection{SNF} \label{dataInt_SNF}
The SNF~\cite{wang2014similarity} approach uses networks of samples as basis for integration. It consists of two steps:
\begin{itemize}
    \item construct a sample-similarity network for each data type
    \item integrate the networks into a single similarity network using a nonlinear combination method
\end{itemize}
The fused network captures shared and complementary information from the different data sources, hus offering insight into how informative each data view is to the similarity between samples.

The first step in the SNF algorithm is the construction of a similarity matrix among samples for each data source $s$, as described in Section~\ref{methods_dataInt}. Then, other two matrices are derived from $W^{(s)}(i,j)$. One is a ``global" similarity matrix $P^{(s)}$ which captures the overall relationships between samples:

\begin{equation*}
    G^{(s)}(i,j) = 
    \begin{dcases}
        \frac{W^{(s)}(i,j)}{2 \sum_{k \neq i} W^{(s)}(i,k)} & \text{ if $j \neq i$}\\
        \frac{1}{2} & \text{ if $j = i$}\\
    \end{dcases}       
\end{equation*}


For this equation the property $\sum_{j} P(i,j)=1$ holds.

The other one is a ``local" similarity matrix $S^{(s)}$, that considers only local similarities in the neighbourhood of each individual:

\begin{equation*}
    L^{(s)}(i,j) = 
    \begin{dcases}
      \frac{W^{(s)}(i,j)}{\sum_{k \in N_i} W^{(s)}(i,k)} & \text{ if $j \in N_i$}\\
      0 & \text{otherwise}\\
    \end{dcases}       
\end{equation*}

where $N_i = \{ x_k | x_k \in \text{kNN}(x_i) \cup \{ x_i \}\}$. \newline


For all $s\in S$ data views, the matrices $W^{(s)}$, $L^{(s)}$ and $G^{(s)}$ are constructed. The similarities are diffused through the $G^{(s)}$s until convergence;this occurs when matrix in step $T$ is similar to the one computed in step $T-1$.  
In the simplest case, when $|S|=2$, we have $G_t^{(s)}$ that refers to the matrix $P^{(s)}$ for data $s \in \{ s_1,s_2\}$ at time $t$.
For $|S|=2$ the following recursive updating formulas describe the diffusion process:

\begin{equation*}
    \begin{aligned}
        G^{(s_1)}_{t+1}=L^{(s_1)} \times G^{(s_2)}_{t} \times L^{(s_1)\top} \\
        G^{(s_2)}_{t+1}=L^{(s_2)} \times G^{(s_1)}_{t} \times L^{(s_2)\top}  
    \end{aligned}
\end{equation*}

In other words $G^{(s_1)}$ is updated by using $L^{(s_1)}$ from the same data source but $G^{(s_2)}$ from a different view and vice-versa.

SNF can be easily extended to $|S|>2$ data sources. \newline


The final integrated matrix $G^{(c)}$ is the average of all the global matrices that have reached convergence:

\begin{equation*}
    G^{(c)} = \frac{1}{|S|} \sum_{k=1}^{|S|} G_T^{(k)}
\end{equation*}


\section{Clustering algorithms}\label{methods_clustering}
As explained in the Introduction (Chapter~\ref{introduction}) we use clustering algorithms to classify the samples in the multi-omics dataset into one of the three disease subtypes found with iCluster in~\cite{shen2009integrative}. We hope to find a classification similar to the one proposed in the cited paper. We have chosen as clustering methods the PAM algorithm%~\cite{PAM}
, illustrated in Section~\ref{clustering_PAM}, and spectral clustering~\cite{von2007SP}, as discussed in Section~\ref{clustering_spectral}.

All approaches are based on the same principle: when partitioning a set of objects into $k$ clusters, the main objective is to group objects with a high degree of similarity in the same cluster, while dissimilar objects end up in different clusters.

\subsection{Partitioning Around Medoids (PAM)}\label{clustering_PAM}
This method is based on finding $k$ representative objects, called medoids, among the objects in the dataset. These objects are considered representative if they model various aspects of the structure of the data. Medoids are defined as the objects of the cluster for which the average dissimilarity to all the objects of the cluster is minimal. Subsequently the $k$ clusters are constructed by assigning each element of the dataset to the nearest medoid. The final goal is to obtain a set of clusters where the average or the sum distances of objects belonging to the cluster and the medoid is minimized.

The data given in input to PAM can assume two possible configurations:
\begin{description}
    \item[measurement values matrix] where the rows represent objects and the columns correspond to variables hte represent coordinates (all contained in a certain interval),
    \item[dissimilarity matrix] where the cells of the matrix indicate the distance between the objects specified in the corresponding row and column.
\end{description}
In our study we have opted for the second approach, since it is easy to express the distance between objects as a function of their similarity obtained though integration of the data. Specifically, for $S$ being the similarity matrix of the objects, the $D$ distance matrix is the result of applying the following function :
\begin{equation*}
    S_{i,j} \mapsto 1 - \text{\textsl{min\_max\_norm}}(S_{i,j})
\end{equation*} 
where $i,j$ are indices of the matrix $S$.

The function \textsl{min\_max\_norm} refers to the normalization of the elements in the affinity matrix $S$ in the range $\left[0, 1\right]$, using min-max normalization. In other words: 
\begin{equation*}
    \text{\textsl{min\_max\_norm}}(S_{i,j}) = \frac{S_{i,j}-\text{min}\{S_{i,j}|\forall i,j\}}{\text{min}\{S_{i,j}|\forall i,j\}-\text{max}\{S_{i,j}|\forall i,j\}}
\end{equation*}
\newline

Now we outline an abstract description of the PAM algorithm.\newline 
For clarity's sake we define $O$ as the set of all objects in the dataset, and $S$ as the set of objects that are tentatively defined as medoids. The derived set $U = O - S$ is the set of unselected objects.

The algorithm has two phases:

\subsubsection*{BUILD phase}
An initial clustering is obtained by the successive selection of the representative objects, until $k$ objects populate the set of selected objects $S$. The first chosen medoid is the object for which the sum of the dissimilarities to all other objects is as small as possible. In other words, it is the object that is most centrally located in $O$. Subsequently, the other $k-1$ representative objects are those that decrease the objective function as much as possible. In particular, a point $i \in U$ is chosen as a representative objects if it has a high number of unselected objects $j\in U\setminus \{i\}$ that are closer to $i$ than to already selected representatives belonging to $S$.

\subsubsection*{SWAP phase}
This phase is intended to improve the set of selected representatives in $S$.

For each pair $(i,h)$, where $i \in S$ is representative and $h \in U$ is non-representative, we execute the following procedure.
\begin{enumerate}
    \item Swap $i$ and $h$, as that $h$ is a representative and $i$ is not.\label{item:1}
    \item Compute the contribution $C_{jih}$ of each object $j \in U\setminus \{h\}$ to the swap of $i$ and $h$. With $E_j$ being the dissimilarity between $j$ and the second most similar representative object in $S$, and $D_{max,j} = \max_{k\in S}{d(j,k)}$, $C_{jih}$ si defined as follows:
    \begin{equation*}
        C_{jih}=
        \begin{dcases}
            0 & \text{if } d(j,i)> D_{max,j}\land d(j,h)> D_{max,j}\\
            d(j,h)-d(j,i) & \text{if } d(j,h)<E_j  \\
            E_j-d(j,i) & \text{if } d(j,h)\geq E_j \\
        \end{dcases}
    \end{equation*}  
    \item Calculate the total result of a swap between $i$ and $h$ by adding all contributions $C_{jih}$
    \begin{equation*}
        T_{ih}=\sum_j{C_{jih}}
    \end{equation*}
    \item Determine the pair $(i,h)$ such that
    \begin{equation*}
        (i,h)=\argmax_{(i,h)}{T_{ih}}
    \end{equation*}
    \item 
    \begin{description}
        \item[If \boldmath$T_{ih}<0$] swap $i$ and $h$ and return to step~\ref{item:1}.
        \item[If \boldmath$T_{ih}\geq 0$] the algorithm terminates because the objective function cannot be decreased by carrying out a swap.
    \end{description}
\end{enumerate}
Seen that all valid swaps are considered, the results of the algorithm do not depend on the order in which the objects are taken into account.

\subsection{Spectral clustering}\label{clustering_spectral}
For this approach we look at the similarity graph where the objects of the matrix correspond to the nodes of the graph. In our case the input of the algorithm is a similarity matrix, that acts as the weight matrix for the graph. The clustering problem in this case is reformulated using the similarity graph. We want to find a partition of the graph such that the edges between different groups have low weights and edges within the subgraph have high weights. In other words, the objects in a cluster that correspond to nodes in the same group are similar, while the points in different clusters are dissimilar from each other.

We present in Algorithm~\ref{alg: spectral_clustering} one version of spectral clustering~\cite{Ng2002spectralClustering}, in particular the one applied to our multi-omics dataset objects. The main trick is to change the representation of the abstract data points $x_i$, for which the similarity is expressed in the affinity matrix given as input, into points $y_i\in\mathbb{R}^k$. This change of representation is useful thanks to the properties of the graph Laplacians. In particular, it enhances the cluster-properties in the data, so that the cluster can be trivially detected in the new representation.
\begin{algorithm}
    \caption{Spectral clustering}
    \label{alg: spectral_clustering}
    \begin{algorithmic}[1]
        \Input
        \Statex $S\in \mathbb{R}^{n\times n}$ similarity matrix
        \Statex $k$ number of clusters to construct
        \EndInput
        \Statex
        \State Compute a similarity graph, let $W$ be it's weighted adjacency matrix \label{alg: spectral_clustering: line2}
        \State Compute the unnormalized Laplacian $L$ \label{alg: spectral_clustering: line3}
        \State Compute the $k$ eigenvectors $u_1,\dots,u_k$ for $L$, corresponding to the $k$ smallest eigenvalues\label{alg: spectral_clustering: line4}
        \State Let $U\in \mathbb{R}^{n\times k}$ with $U=\left[u_1|u_2|\dots|u_k\right]$\label{alg: spectral_clustering: line5}
        \State $\forall i \in \{1,\dots,n\}$ let $y_i\in \mathbb{R}^k$ be the vector corresponding to the $i$-th row of U\label{alg: spectral_clustering: line6}
        \State Cluster the points $\{y_i\}_{i\in\{1,\dots,n\}}$ in $\mathbb{R}^k$ with $k$-means into clusters $C_1,\dots,C_k$\label{alg: spectral_clustering: line7}
        \Statex
        \Output
        \Statex Clusters $A_1,\dots,A_k: A_i=\{j|y_j\in C_i\}$
        \EndOutput
    \end{algorithmic}
\end{algorithm}
\subsubsection*{Normalized graph Laplacian}
The unnormalized graph Laplacian is defined as
\begin{equation*}
    L\coloneqq D-W
\end{equation*}
where
\begin{description}
    \item[\boldmath$D$] is the degree matrix, defined as the diagonal matrix with degrees of each node on the diagonal,
    \item[\boldmath$W$] is the weighted adjacency matrix of the graph.    
\end{description}

First of all we present one of the main properties of such matrix. For other properties of interest, consult~\cite{von2007SP}.
\begin{proposition}[Property of $L$]\label{prop:Lprop}
    $L$ is positive semi-definite and has $n$ non-negative, real-valued eigenvalues such that $0=\lambda_1\leq\dots\leq\lambda_n$.
\end{proposition}
\begin{proof}
    (We refer to~\cite{von2007SP} for the proof of this proposition.)
\end{proof}

The unnormalized graph Laplacian and its eigenvalues and eigenvectors can be used to describe properties of the graph, one of which is important in the realm of spectral clustering.
\begin{proposition}[Number of connected components and spectra of $L$]\label{prop:LnumberConnectedComponents}
    Let $G$ be and undirected graph with non-negative weights. Then the multiplicity $k$ of the eigenvalue 0 of $L$ equals the number of connected components $A_1,\dots,A_k$ in the graph. The eigenspace of 0 is spanned by the indicator vectors $\mathds{1}_{A_1},\dots,\mathds{1}_{A_k}$, where $\mathds{1}_{A_i}$ is the indicator vector of $A_i$.
\end{proposition}
\begin{proof}
    (We refer to~\cite{von2007SP} for the proof of the proposition.)
\end{proof}

Proposition~\ref{prop:Lprop} and Proposition~\ref{prop:LnumberConnectedComponents} allow us to conclude, logically, that there exists at least one connected component in the similarity graph taken into consideration, seen that at least one of the eigenvalues of $L$ is 0. Looking at Algorithm~\ref{alg: spectral_clustering}, in line~\ref{alg: spectral_clustering: line3} we compute the graph Laplacian for the similarity graph extracted from the input matrix $S$, and in the following step (line~\ref{alg: spectral_clustering: line4}) we extract the eigenvectors for $L$. It is important to note that only the first $k$ eigenvectors that correspond to the $k$ smallest eigenvalues are looked at. Supposing that all of the $k$ eigenvalues of $L$ where to be 0, the eigenvalues $u_1,\dots,u_k$ extracted in line~\ref{alg: spectral_clustering: line4} are the indicator vectors of $k$ connected components in the similarity graph, as stated in Proposition~\ref{prop:LnumberConnectedComponents}. Seen that the connected components are at least $k$, the matrix $U$, defined in line~\ref{alg: spectral_clustering: line5}, has exactly the same rows for the corresponding nodes in a given component. As a consequence, if we consider the row vectors $\{y_i\}_{i\in\{1,\dots,n\}}$ of $U$ (see line~\ref{alg: spectral_clustering: line6}), it is sufficient to cluster them by grouping the equivalent vectors (line~\ref{alg: spectral_clustering: line7}). Seen that the problem is already solved thanks to the properties of the graph Laplacian we can use a simple clustering algorithm; in this case the authors have chosen the simple $k$-means algorithm~\cite{Hastie2009ElementsStatisticalLearning}.

For fixed $k$ desired amount of clusters, we have three cases depending on the number of connected components in the similarity graph $c$:
\begin{description}
    \item[if \boldmath$k=c$] the resulting clusters $C_1,\dots,C_k$ each contain exactly the nodes of a distinct connected component,
    \item[if \boldmath$k<c$] some connected components collapse into the same cluster
    \item[if \boldmath$k>c$] all the objects in a cluster belong to the same component in the similarity graph, but the nodes of at least one of the connected components will be spread across distinct clusters according to other similar traits that emerge among the nodes in the component. These characteristics depend on the the eigenvectors of $L$ for which the corresponding eigenvalues are strictly positive (eigenvalues are non-negative for Proposition~\ref{prop:Lprop}).
\end{description}
In the similarity graph we could have pseudo connected components, for which the edges connecting a subset of nodes to the rest of the graph have weights near to 0. In this case, though such group of nodes is technically not a connected component, one of the eigenvectors will have an aspect similar to an indicator vector for such nodes because the eigenvalue will be close to 0. We may generalize such reasoning to pseudo connected components in which the bridging edges have weights that are noticeably smaller that the weights among the nodes in the group. Seen that such nodes will be group together, we arrive at a acceptable solution of the graph partitioning problem, as presented at the beginning of Section~\ref{alg: spectral_clustering}.