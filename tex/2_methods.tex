\chapter{Methods}
\section{Data pre-processing}\label{methods_preProcessing}
In this section we list all the modifications that have been made to the original multi-omics dataset downloaded from TCGA.\newline

For the following paragraphs it is helpful to understand the format of TCGA barcodes\footnote{More information about TCGA barcodes can be found at \url{https://docs.gdc.cancer.gov/Encyclopedia/pages/TCGA_Barcode/}}, that describe the nature of the samples. These barcodes contain a set of codes that specify various information about the relative sample. A barcode has 28 characters that describe:
\begin{equation*}
    \underbrace{\text{\footnotesize AAAA}}_\text{project}-
    \underbrace{\text{00}}_\text{TSS}-
    \underbrace{\text{0000}}_\text{participant}-
    \underbrace{\text{00}}_\text{sample}
    \underbrace{\text{A}}_\text{vial}-
    \underbrace{\text{00}}_\text{portion}
    \underbrace{\text{A}}_\text{analyte}-
    \underbrace{\text{0000}}_\text{plate}-
    \underbrace{\text{00}}_\text{center}
\end{equation*}
\paragraph{Primary solid tumors}
We only look at samples regarding primary solid tumors, so those that do not concern e.g. metastatic, solid, blood derived tumors. The type of sample analysed is specified by the sample barcode characters ``\texttt{01}" in position 13 and 14.
For the dataset in question the length of the barcodes is one character shorter for the data view \textit{PRAD\_RPPAArray}, in fact the specification of analyte information for the samples is missing in the 20th position of the barcode. Seen that in the evaluation phase we consider only the first 12 characters of the barcode, concerning a specific patient (fields Project, Tissue Source Site (TSS) and Participant), this difference is irrelevant for the scope of this project.
\paragraph{Replicates} We do not want to avoid working with replicates for the same sample, so we check that the barcode, the identifier for the sample, are unique in the multi-omics dataset. It emerges that our dataset doesn't contain replicated samples.
\paragraph{FFPE} Also, we want to take into account only samples which biopsies have been frozen. In fact, formalin-fixed, paraffin-embedded (FFPE) biopsies are preserved worse.
\paragraph{All data views} Obviously we want to look at the samples from all three points of view, so we keep only those that are present in all three considered data views in the multi-omics dataset.
\paragraph{Missing values} In addition we get rid of features having missing values. The samples kept until now in the mRNA and miRNA data views don't contain \textit{NA}s, but the protein expression features were not completely ``clean''.\
\paragraph{Highest variance} Subsequently we select features having the most variance across samples. The idea is that such data brings more information, and so is more relevant. In this case we consider only the first 100 features with highest variance between the samples.
\paragraph{Normalization} We then standardize such features with z-score, by applying for each value $v$
\begin{equation}
    \frac{(v-\mu)}{\sigma}
\end{equation}
where $\mu$ and $\sigma$ are respectively the mean and standard deviation of the values for a same attribute across samples.
\paragraph{Trim barcodes} We are only interested in the barcode information about a specific individual, so we retain only the first 12 characters describing the patient. In particular we keep the sample's Project, TSS and Participant.
\paragraph{Samples with specified subtype} We consider only the samples in the multi-omics that possess information about the subtype data obtained through iCluster. Seen that the subtype samples are a subset of those in the multi-omics data, it is sufficient to keep only that subset of patients.

\section{Data integration}\label{methods_dataInt}
We could try and classify each data view of the multi-omics dataset (in our case mRNA, miRNA and protein expression) into subtypes. Nevertheless the disease subtpyes would be better determined if all of the molecular data were taken into account simultaneously. So, we want to fuse the different types of data from the multi-omics dataset into a unique object on which it is possible to evaluate the distance between the samples. We start by finding the similarity matrix for each data source using exponential Euclidian distance, also called an affinity matrix.
More formally the similarity matrix among samples of a data source $s$ is given by:
\begin{equation} \label{eq:scaled_exponential_sim}
    W^{(s)}(i,j) = exp \left(- \frac{\rho(x_i,x_j)^2}{\mu \varepsilon_{ij}}\right)
\end{equation}
where:
\begin{description}
    \item [$\rho(x_i, x_j)$] is the Euclidean  distance  between samples \(x_i\) and \(x_j\)
    \item [$\mu$] is a parameter
    \item [$\varepsilon_{i,j}$] is a scaling factor: $\varepsilon_{i,j} = \frac{mean(\rho(x_i, N_i)) + mean(\rho(x_j, N_j)) + \rho(x_i, x_j)}{3}$, where  $mean(\rho(x_i, N_i))$ is the average value of the distances between $x_i$ and each of its neighbours
\end{description}
These similarity matrices for each data source $s$ specify the distance between all possible pairs of samples. This format of the data allows the application of data integration methods, like the ones described in Section \ref{dataInt_mean} and \ref{dataInt_SNF}.

\subsection{Mean} \label{dataInt_mean}
In this case we compute the integrated matrix by averaging component-wise the original three distance matrices obtained from data regarding mRNA, miRNA and protein espression. This method is very naÃ¯ve since it considers strictly the similarity between pairs of samples, without having a global view of the distances.

\subsection{SNF} \label{dataInt_SNF}
The SNF\cite{wang2014similarity} approach uses networks of samples as basis for integration. It consists of two steps:
\begin{itemize}
    \item construct a sample-similarity network for each data type
    \item integrate the networks into a single similarity network using a nonlinear combination method
\end{itemize}
The fused network captures shared and complementary information from the different data sources, hus offering insight into how informative each data view is to the similarity between samples.

The first step in the SNF algorithm is the construction of a similarity matrix among samples for each data source $s$, as described in Section \ref{methods_dataInt}. Then, other two matrices are derived from $W^{(s)}(i,j)$. One is a ``global" similarity matrix $P^{(s)}$ which captures the overall relationships between samples:

\begin{equation}\label{eq:global_kernel}
    G^{(s)}(i,j) = 
    \begin{dcases}
        \frac{W^{(s)}(i,j)}{2 \sum_{k \neq i} W^{(s)}(i,k)} & \text{ if $j \neq i$}\\
        \frac{1}{2} & \text{ if $j = i$}\\
    \end{dcases}       
\end{equation}


For this equation the property $\sum_{j} P(i,j)=1$ holds.

The other one is a ``local" similarity matrix $S^{(s)}$, that considers only local similarities in the neighbuorhood of each individual:

\begin{equation}
\label{eq:local_kernel}
    L^{(s)}(i,j) = 
    \begin{dcases}
      \frac{W^{(s)}(i,j)}{\sum_{k \in N_i} W^{(s)}(i,k)} & \text{ if $j \in N_i$}\\
      0 & \text{otherwise}\\
    \end{dcases}       
\end{equation}

where $N_i = \{ x_k | x_k \in kNN(x_i) \cup \{ x_i \}\}$. \newline


For all $s\in S$ data views, the matrices $W^{(s)}$, $L^{(s)}$ and $G^{(s)}$ are constructed. The similarities are diffused through the $G^{(s)}$s until convergence;this occurs when matrix in step $T$ is similar to the one computed in step $T-1$.  
In the simplest case, when $|S|=2$, we have $G_t^{(s)}$ that refers to the matrix $P^{(s)}$ for data $s \in \{ s_1,s_2\}$ at time $t$.
For $|S|=2$ the following recursive updating formulas describe the diffusion process:

\begin{equation}
    \label{eq:update}
    \begin{aligned}
        G^{(s_1)}_{t+1}=L^{(s_1)} \times G^{(s_2)}_{t} \times L^{(s_1)\top} \\
        G^{(s_2)}_{t+1}=L^{(s_2)} \times G^{(s_1)}_{t} \times L^{(s_2)\top}  
    \end{aligned}
\end{equation}

In other words $G^{(s_1)}$ is updated by using $L^{(s_1)}$ from the same data source but $G^{(s_2)}$ from a different view and vice-versa.

SNF can be easily extended to $|S| > 2$ data sources. \newline


The final integrated matrix $G^{(c)}$ is the average of all the global matrices that have reached convergence:

\begin{equation}
    \label{eq:consensus}
    G^{(c)} = \frac{1}{|S|} \sum_{k=1}^{|S|} G_T^{(k)}
\end{equation}


\section{Clustering algorithms}\label{methods_clustering}
\subsection{PAM}\label{clustering_PAM}
\subsection{Spectral clustering}\label{clustering_spectral}