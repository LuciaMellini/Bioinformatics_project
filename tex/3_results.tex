\chapter{Results}

In Sections~\ref{results_pam} and~\ref{results_sc} we present the results for each clustering approach, depending on the used integration methods. We give a qualitative evaluation of the possible partition in subtypes of the samples by means of plots. The points represent the samples as rendered in Euclidean space after potential integration. They are extracted with Multidimensional Scaling from the similarity or distance matrices, depending on the clustering approach. The axes of the plots are labeled as \textit{Component1} and \textit{Component2} seen that the similarity and the distance are measured between pairs of objects in the multi-omics dataset.

Also quantitative measures are provided by comparing the clusterings to the subtypes assigned by iCluster through multiple indices selected from~\cite{wagner2007comparing}. The chosen indices to measure cluster quality, subdivided by type, are the following:
\begin{itemize}
    \item Counting pairs
    \begin{itemize}             
        \item Rand index
        \item Adjusted Rand index
        \item Jaccard index
    \end{itemize}
    \item Set overlaps
    \begin{itemize}
        \item $\mathcal{F}$-measure
    \end{itemize}
    \item Mutual information
    \begin{itemize}
        \item Joint entropy
no        \item Normalized mutual information (by Strehl \& Ghosh)
    \end{itemize}
\end{itemize}
To better grasp the meaning of the results we examine these indices in depth in Section~\ref{results_indices}.





\section{Clustering quality indices}\label{results_indices}
We would like to answer the question of how similar the solutions of two different clustering algorithms are. In the next sections we refer to $X$ a finite set of objects such that $|X|=n$. A clustering $\mathcal{C}$ is a set $\{C_1,\dots,C_k\}$ that is a partition of $X$.

\subsection{Counting pairs}
To compare clusterings $\mathcal{C}$ and $\mathcal{C}'$ we can look at pairs of elements classified in the same way in both clusterings. So, on the basis of it partition according to a certain clustering, the set of objects $X$ can be seen as the disjoint union of the following sets:
\begin{equation*}
    \begin{gathered}
        S_{11}=\{(s,r)|(s\in C_i\land r\in C_i) \land (s\in C'_j\land r\in C'_j)\}\\
        S_{00}=\{(s,r)|(s\in C_i\land r\notin C_i) \land (s\in C'_j\land r\notin C'_j)\}\\
        S_{10}=\{(s,r)|(s\in C_i\land r\in C_i) \land (s\in C'_j\land r\notin C'_j)\}\\
        S_{01}=\{(s,r)|(s\in C_i\land r\notin C_i) \land (s\in C'_j\land r\in C'_j)\}
    \end{gathered}
\end{equation*}
Let $n_{ab}\coloneqq|S_{ab}|$, $a,b\in\{0,1\}$ denote the respective sizes.
\subsubsection{Rand index}
In this case we count the correctly classified elements. Thus, the Rand index si defined by:
\begin{equation*}
    \mathcal{R}(\mathcal{C},\mathcal{C}')=frac{2(n_{11}+n_{00})}{n(n-1)}
\end{equation*}
$mathcal{R}$ ranges from 0, when the clusterings are identical, to 1, when the clusterings are identical. Seen that the similarity index also depends on the number of clusterings, it can be shown that unfortunately in the case of independent clusterings the Rand Index converges to 1 the number of clusterings increase.

\subsubsection{Adjusted Rand index}
When working with two random clusterings the expected value of the Random index is not a constant. The Adjusted Random index is the normalized difference between the Rand index and its expected value under a null hypothesis. The adjustment assumes a generalized hypergeometric distribution as a null hypothesis: the two clusterings are drawn randomly each with a fixed number of clusters of fixed size. 
The index is defined as follows:
\begin{equation*}
    \begin{gathered}
       \mathcal{R}_{adj}(\mathcal{C},\mathcal{C}')=\frac{\sum_{i=1}^k\sum_{j=1}^\ell\binom{|C_i\cap C'_j|}{2}-t_3}{\frac{1}{2}(t_1+t_2)-t_3} \\
       \text{where } t_1=\sum_{i=1}^k\binom{|C_i|}{2}\text{, }t_2=\sum_{j=1}^\ell\binom{|C'_j|}{2}\text{, }t_3=\frac{2t_1t_2}{n(n-1)}
    \end{gathered}    
\end{equation*}
$\mathcal{R}_{adj}$ has maximum value 1 for identical clusterings. The index has expected value 0 for independent clusterings, but some pairs of clusterings distributed differently with respect to the null assumption may result in negative values, up to $-0.5$.

\subsubsection{Jaccard index}
The Jaccard index is similar to the Rand index, but it disregards the pairs of elements that are in different clusters for both clusterings. It is defined as follows:
\begin{equation*}
    \mathcal{J}(\mathcal{C},\mathcal{C}')=\frac{n_{11}}{n_{11}+n_{01}+n_{10}}
\end{equation*}

\subsection{Set overlaps}
These kinds of measure try to match clusters that have a maximum or relative overlap.

\subsubsection{$\mathcal{F}$-measure}
The $\mathcal{F}$-measure for a cluster $C'_j$ with respect to a class $C_i$ indicates how good the cluster $C'_j$ describes $C_i$ in the following way:
\begin{equation*}
    \begin{gathered}
        \mathcal{F}(C_i,C'_j)=\frac{2\cdot r_{ij}\cdot p_{ij}}{r_{ij}+p{ij}}=\frac{2|C_i||C'_j|}{|C_i|+|C'_j|}\\
        \text{where }p_{ij}=\frac{|C_i\cap C'_j|}{|C'_j|} \text{ (mean of precision) and } r_{ij}=\frac{|C_i\cap C'_j|}{|C_i|} \text{ (recall)}
    \end{gathered}
\end{equation*}

The overall $\mathcal{F}$-measure is defined as the weight of the maximum $\mathcal{F}$-measures fot the clusters in $\mathcal{C}'$:
\begin{equation*}
    \mathcal{F}(\mathcal{C},\mathcal{C}')=\mathcal{F}(\mathcal{C}') = \frac{1}{n}\sum_{i=1}^k n_i\max_{j=1}^\ell\{\mathcal{F}(C_i,C'_j)\}
\end{equation*}

This measure is not symmetric since it indicates the quality of a clustering in relation with another. So it may be appropriate to compare a clustering with an optimal clustering solution. In our case we tale the partition in disease subtypes extracted with iCluster as our baseline.

\subsection{Mutual information}
This approach in the comparison of clusterings has its origin in information theory, and is based on the notion of entropy. Let's assume all elements in $X$ have the same probability of being picked. Then, choosing an element of $X$ at random, the probability that this element is in cluster $C_i\in\mathcal{C}$ is $\mathbb{P}(i)=\frac{|C_i|}{n}$. The entropy associated with clustering $\mathcal{C}$ iCluster
\begin{equation*}
    \mathcal{H}(\mathcal{C})=-\sum_{i=1}^k\mathbb{P}(i)\log_{2}{(\mathbb{P}(i))}.
\end{equation*}

In other words the entropy of a clustering $\mathcal{C}$ measures the uncertainty about the cluster assigned to a randomly chosen element in $X$. As a consequence, the entropy is 0 for a trivial clustering, that groups all elements in one subset or partitions $X$ by separating each element.

\subsubsection{Joint entropy}
The notion of entropy can be extended to that of mutual information between two clusterings $mathcal{C}$, $\mathcal{C}'$, formally defined as
\begin{equation*}
    \begin{gathered}
       \mathcal{MI}(C_i, C_j) = \sum_{i=1}^{k} \sum_{j=1}^{l} \mathbb{P}(i,j) \log_{2}{ \frac{\mathbb{P}(i,j)}{\mathbb{P}(i)\mathbb{P}(j)} }\\
       \text{where }\mathbb{P}(i,j)=\frac{|C_i\cap C'_j|}{n}
    \end{gathered}    
\end{equation*}
so $\mathbb{P}(i,j)$ denotes the probability that an element belongs to $C_i\in\mathcal{C}$ and to cluster $C'_j\in\mathcal{C}'$.

Conceptually, it describes how much we can on average reduce the uncertainty about the cluster of a random element in $X$ when knowing its cluster in another partition of $X$.

This metric is not bounded by a constant value since
\begin{equation*}
    \mathcal{MI}(\mathcal{C},\mathcal{C}')\leq\min\{\mathcal{H}(\mathcal{C})\mathcal{H}(\mathcal{C}')\}
\end{equation*}
which makes it a difficult measure to interpret. So, it seems reasonable to apply a normalization by the geometric or arithmetic mean of the entropies. In the next section we present the former, proposed in~\cite{StrehlA2003Ce-A}.

\subsubsection{Normalized mutual information (by Strehl \& Gosh)}
We would like to approximately determine the clustering that has maximal average normalized mutual information with all the clustering in consideration. So we look at the normalized mutual information between two clusterings defined as
\begin{equation*}
    \mathcal{NMI}(C_i, C_j) = \frac{\mathcal{MI}(C_i, C_j)}{\sqrt{\mathcal{H}(C_i) \mathcal{H}(C_j)}}
\end{equation*}.
Seen that $\mathcal{H}(\mathcal{C})=\mathcal{MI}(\mathcal{C},\mathcal{C})$, the geometric mean is the preferred normalization. So, the normalized mutual information measure ranges between 0, when $\mathbb{P}(i,j)=0$ or $\mathbb{P}(i,j)=\mathbb{P}(i)\cdot\mathbb{P}(j)$, and 1 when $\mathcal{C}=\mathcal{C}'$.

\section{Clustering with PAM}\label{results_pam}
\subsection{Non-integrated data}\label{pam_nonIntegrated}
\begin{figure}\label{fig:pam_1}
    \centering
    \resizebox{0.45\textheight}{!}{\subimport{./plots}{pam_1.tex}}
    \caption{Clustering with PAM - non-integrated data - mRNA view}
\end{figure}
\begin{figure}\label{fig:pam_2}
    \centering
    \resizebox{0.45\textheight}{!}{\subimport{./plots}{pam_2.tex}}
    \caption{Clustering with PAM - non-integrated data - miRNA view}
\end{figure}
\begin{figure}\label{fig:pam_3}
    \centering
    \resizebox{0.45\textheight}{!}{\subimport{./plots}{pam_3.tex}}
    \caption{Clustering with PAM - non-integrated data - protein expression view}
\end{figure}

\subsection{Data integration though mean}\label{pam_mean}
\begin{figure}\label{fig:pam_mean}
    \centering
    \resizebox{0.45\textheight}{!}{\subimport{./plots}{pam_mean.tex}}
    \caption{Clustering with PAM - data integrated through mean}
\end{figure}

\subsection{Data integration though SNF}\label{pam_SNF}
\begin{figure}\label{fig:pam_SNF}
    \centering
    \resizebox{0.45\textheight}{!}{\subimport{./plots}{pam_SNF.tex}}
    \caption{Clustering with PAM - data integrated through SNF}
\end{figure}

\section{Spectral clustering}\label{results_sc}
\subsection{Non-integrated data}\label{sc_nonIntegrated}
\begin{figure}\label{fig:sc_1}
    \centering
    \resizebox{0.45\textheight}{!}{\subimport{./plots}{sc_1.tex}}
    \caption{Spectral clustering - non-integrated data - mRNA view}
\end{figure}
\begin{figure}\label{fig:sc_2}
    \centering
    \resizebox{0.45\textheight}{!}{\subimport{./plots}{sc_2.tex}}
    \caption{Spectral clustering - non-integrated data - miRNA view}
\end{figure}
\begin{figure}\label{fig:sc_3}
    \centering
    \resizebox{0.45\textheight}{!}{\subimport{./plots}{pam_3.tex}}
    \caption{Spectral clustering - non-integrated data - protein expression view}
\end{figure}

\subsection{Data integration though mean}\label{sc_mean}
\begin{figure}\label{fig:sc_mean}
    \centering
    \resizebox{0.45\textheight}{!}{\subimport{./plots}{sc_mean.tex}}
    \caption{Spectral clustering - data integrated through mean}
\end{figure}

\subsection{Data integration through SNF}\label{sc_SNF}
\begin{figure}\label{fig:sc_SNF}
    \centering
    \resizebox{0.45\textheight}{!}{\subimport{./plots}{sc_SNF.tex}}
    \caption{Spectral clustering - data integrated through SNF}
\end{figure}